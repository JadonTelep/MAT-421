{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLN8DjTkvIkqrYIMkOxozJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JadonTelep/MAT-421/blob/main/SP_24_MAT_421_Module_D_Root_Finding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Module D - Linear Equations and Regression**"
      ],
      "metadata": {
        "id": "qLAE0l9EyWpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.1 (Introduction)"
      ],
      "metadata": {
        "id": "qFtC8dlRpp6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear algebra is a field of mathematics that is widely used in various disciplines. Linear algebra plays an important role in data science and machine\n",
        "learning. A solid understanding of linear algebra concepts can enhance the\n",
        "understanding of many data science and machine learning algorithms. This\n",
        "chapter introduces basic concepts for data science and includes vector spaces,\n",
        "orthogonality, eigenvalues, matrix decomposition and further expanded to include linear regression and principal component analysis where linear algebra\n",
        "plays a central role for solving data science problems."
      ],
      "metadata": {
        "id": "aoOcL5uQpte1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.2 (Elements of Linear Algebra)"
      ],
      "metadata": {
        "id": "nkMEYP6eW-hC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1.2.1 (Linear Spaces)"
      ],
      "metadata": {
        "id": "MX8xqHD8qkYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this secition, we will be workng in the Euclidean Space $V = ‚Ñù^n$\n",
        "\n",
        "Where $V$ is a **linear space**, a **linear subspace**, $U$,is a subset $U ‚äÜ V$ that is closed under vecotr addition and scalar multiplications. That is to say for $u_1,u_2 ‚àà U$ and $\\alpha ‚àà ‚Ñù$ it holds that\n",
        "\n",
        "$u_1 + u_2$ and $\\alpha u_1 \\in U$\n",
        "\n",
        "In particular, **0** is always in a linear subspace."
      ],
      "metadata": {
        "id": "JGH1lyekXD68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **span** is the set of all linear combinations in $V$. Every span is a linear subspace and can be written as such\n",
        "\n",
        "span$(w_1,...,w_m) = \\{ \\sum_{j=1}^{m} \\alpha_jw_j: \\alpha_1,...,\\alpha_m \\in ‚Ñù\\}$\n",
        "\n",
        "We can set a linear subspace to be a span and vice versa.\n",
        "\n",
        "$u_1,u_2 ‚àà W ‚áî u_i =\\sum_{j=1}^{m} Œ≤_i,jw_j$\n",
        "\n",
        "Lastly, a **column space** denoted col$(A)$, is the span of the columns of $A$"
      ],
      "metadata": {
        "id": "m0sPbuH3c4Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is desirable to avoid redundancy in a linear subspace. **Linear indepenence** is a linear subspace that cannot be writtent as a linear combination of the others.\n",
        "\n",
        "That is to say for vectors $u_1,...,u_m$ are linearly independent iff\n",
        "\n",
        "$\\sum_{j=1}^{m} \\alpha_ju_j = 0 ‚üπ \\alpha_j = 0, ‚àÄj$\n",
        "\n",
        "Now if $U$ is a linear subspace of $V$, then a **basis** of $U$ is a span $U$ that is linearly independent. We also refer to a span of $U$ as the **spanning set**."
      ],
      "metadata": {
        "id": "C3vEl-1Hc4nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Dimension Theorem** states that for $U$ any basis of of $U$ has the same number of elements. This is refered to as the **dimension** of $U$ or dim$(U)$.\n",
        "\n",
        "The lemma **Characterization of linearly Dependent Sets** states that any linearly dependent set that has a linearly independent subset has a reduced form and can be simplified to the linearly independent set.  \n",
        "\n"
      ],
      "metadata": {
        "id": "zIVbmRgRfQTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1.2.2 (Orthogonality)"
      ],
      "metadata": {
        "id": "kEgib0mlqwJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **inner product** refers to the space between two vectors denoted by $\\langle u,v \\rangle = u \\cdot v = \\sum_{i}^{n} u_i v_i$.\n",
        "\n",
        "The **norm** of the vector refers to the magnitude of that vector $u$ which is $ \\|u\\| = \\sqrt{\\sum_{1}^{n} u_i^2}$\n",
        "\n",
        "A list of vectors$\\{u_1,...,u_m\\}$ is orthonormal if each $u_i$ are pairwise orthogonal and each has a norm of 1. That is to say for all $i$ and all $j\\ne i$, $\\langle u_i,u_j \\rangle = 0$ and $\\|u_i\\| = 1$.\n",
        "\n",
        "**Cauchy-Schwarz** - For any $u,v \\in V, |\\langle u,v \\rangle| \\le \\|u\\|\\|v\\|$"
      ],
      "metadata": {
        "id": "h6mTCyPGp9Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Orthonormal Basis Expansion** - Let $q_1,...,q_m$ be an orthonormal basis of $ùì§$ and let $u \\in ùì§$. Then\n",
        "\n",
        "$u=\\sum^{m}_{j=1} \\langle u,q_j \\rangle q_j$\n",
        "\n",
        "With some vector $u,v$ we can find a vector $v^* = \\alpha^*u$ that will optimaly convert $v$ to be orthogonal to $u$ such that $\\langle u,v-v^* \\rangle = 0$. This translates to\n",
        "\n",
        "$0 = \\langle u,v-v^* \\rangle = \\langle u,v-\\alpha^*u \\rangle = \\langle u,v \\rangle - \\alpha^* \\langle u,u \\rangle = \\langle u,v-v^* \\rangle - \\alpha^*$\n",
        "\n",
        "which implies that $v^* = \\langle u,v \\rangle u$\n",
        "\n",
        "**Projection** - proj$_u(v) = \\frac{v \\cdot u}{\\|u\\|^2}u$ where $\\langle u,$proj$_u(v)$$\\rangle = 0$\n",
        "\n",
        "**Orthogonal Decomposition** - Let $ùì§$ be a subpace of $V$ and let $u \\in ùì§$ and and $v \\in V$. Then each $v$ in $V$ can be uniquely represented in the form $v = vÃÇ + u$ where $vÃÇ \\in ùì§$ . In fact, if $\\{u_1, u_2, ..., u_p\\}$ is any orthogonal basis for $ùì§$, then\n",
        "\n",
        "$vÃÇ =$ proj$_{u_1}(v)$ + proj$_{u_2}(v)$ + ... proj$_{u_p}(v)$ and\n",
        "\n",
        "$ u = v - vÃÇ$"
      ],
      "metadata": {
        "id": "1IFsZ-qNt508"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.3 (Linear Regression)"
      ],
      "metadata": {
        "id": "ssRDspbkpanm"
      }
    }
  ]
}
